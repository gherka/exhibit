# ----------------------------------------------------------
# EXHIBIT SPECIFICATION
# =====================
#
# This specification describes the dataset in great detail.
# In order to vary the degree to which it is anonymised,
# please review each section and make necessary adjustments.
# ----------------------------------------------------------
metadata:
  number_of_rows: 5_000
  uuid_columns:
  - record_chi
  categorical_columns:
  - dose_number
  - sex
  numerical_columns:
  - risk_score
  date_columns:
  - vacc_date
  geospatial_columns:
  inline_limit: 150
  random_seed: 0
  id: demo
# ----------------------------------------------------------
# COLUMN DETAILS
# ==============
#
# Dataset columns are categorised into one of the three types: 
# Categorical | Continuous | Date
#
# Column type determines what parameters are included in the
# specification. When making changes to the values, please
# note their format. Values starting with a number must be
# enclosed in quotes as per YAML rules.
# 
# CATEGORICAL COLUMNS
# -----------
# The default anonymising method for categorical columns is
# "random", meaning original values are drawn at random,
# (respecting probabilities, if supplied) but you can add
# your own custom sets, including linked, by creating a
# suitable table in the anon.db SQLite3 database.
#
# The tool comes with a number of sample anonymising sets
# (see documentation). To use just one column from a set,
# add a dot separator like so mountains.range
# 
# You can also use a subset of regular expression patterns 
# to generate aliasing values in non-linked columns. For example,
# if your confidential data had Consultant GMC numbers, you can
# anonymise them using a regex pattern GMC[0-9]{5}[XY] which
# will generate values like GMC00000X.
#
# Depending on the number of unique values in a column,
# its original values can either be listed in the spec,
# put in the anon.db database or, if the values follow a
# one to one relationship with another column, be listed
# as part of that column's section.
#
# UUID COLUMNS
# -----------
# UUID columns are a special case for categorical column where
# each value is unique, but can appear multiple times depending
# on the frequency distribution set by the user. You can add UUID
# columns manually or infer them from the source data. If adding a
# UUID column manually, don't forget to add it in the metadata section.
#
# The format of a UUID column in the specification is as follows:
# 
# record_chi:
#  type: uuid
#  frequency_distribution:
#  - frequency | probability_vector
#  - 1         | 0.5
#  - 2         | 0.3
#  - 3         | 0.2
#  miss_probability: 0.0
#  anonymising_set: uuid
# ----------------------------------------------------------
columns:
  record_chi:
    type: uuid
    frequency_distribution:
    - frequency | probability_vector
    - 1         | 0.5
    - 2         | 0.3
    - 3         | 0.2
    miss_probability: 0.0
    anonymising_set: uuid
  dose_number:
    type: categorical
    paired_columns:
    uniques: 3
    original_values:
    - dose_number      | probability_vector | risk_score
    - dose_1           | 0.333              | 0.700
    - dose_2           | 0.333              | 0.200
    - dose_3           | 0.333              | 0.100
    - Missing data     | 0.000              | 0.000
    cross_join_all_unique_values: false
    miss_probability: 0.0
    anonymising_set: random
  sex:
    type: categorical
    paired_columns: []
    uniques: 2
    original_values:
    - sex          | probability_vector | risk_score
    - M                | 0.500              | 0.500 
    - F                | 0.500              | 0.500
    - Missing data     | 0.000              
    cross_join_all_unique_values: false
    miss_probability: 0.0
    anonymising_set: random
# ----------------------------------------------------------
# NUMERICAL COLUMNS
# ----------
# Currently, only continuous data is supported. To use numerical
# columns with discrete data, covert them to categorical or make
# sure that the precision parameter is set to integer.
# 
# To generate Continuous columns you have two distribution options:
#
# - weighted_uniform
# - normal
#
# The first option will generate the column by applying weights
# to a fixed value before scaling it and optionally perturb it
# within the dispersion percentage. With dispersion set to zero,
# you can generate identical values for any given combination of
# categorical values on the row.
# 
# A normal distribution will respect categorical weights by
# shifting the mean accordingly. If dispersion is greater than
# zero, the value before scaling is perturbed within the 
# dispersion percentage.
# 
# You can also control how values are scaled by setting the
# distribution target parameters. You must include at least one of
# target_min, target_max, target_sum, target_mean or target_std.
#  
# Note that if you include certain constraints, like ensuring values
# in one column are always greater than values in another column,
# the final scaling of the column being adjusted will be affected.
# ----------------------------------------------------------
  risk_score:
    type: continuous
    precision: float
    distribution: weighted_uniform
    distribution_parameters:
      dispersion: 0.1
      target_min: 10
      target_max: 100
    miss_probability: 0.0
# ----------------------------------------------------------
# DATE COLUMNS
# ----------
# Exhibit will try to determine date columns automatically, but
# you can also add them manually, providing the following parameters:
#   type: date
#   cross_join_all_unique_values: true | false
#   miss_probability: between 0 and 1
#   from: 'yyyy-mm-dd'
#   uniques: 4
#   frequency: QS
# 
# Frequency is based on the frequency strings of DateOffsets.
# See Pandas documentation for more details. Times are not supported.
# ----------------------------------------------------------
# GEOSPATIAL COLUMNS
# ----------
#
# Geospatial columns are special in that they are not inferred
# from source data and can only be added manually. The format 
# of each column is as follows:
#
# clinic_coords:
#    type: geospatial
#    h3_table: geo_scotland_dz_h3_8
#    distribution: uniform
#    miss_probability: 0
#
# h3_table referrs to H3 hexagon IDs stored in anon.db. exhibit
# doesn't come with any geospatial lookups by default, but there
# is a recipe explaining how to create one. The format of the table
# is that it has to have a column named "h3" with h3 ids. 
# 
# For distribution, you can either pick uniform to sample points
# from all hexagons at random or use column weights in the h3_table,
# like population counts.
# ----------------------------------------------------------
  vacc_date:
    type: date
    cross_join_all_unique_values: false
    miss_probability: 0.0
    from: '2020-01-01'
    uniques: 360
    frequency: D
# ----------------------------------------------------------
# CONSTRAINTS
# ===========
#
# There are two types of constraints you can impose of the data:
# - boolean
# - conditional
# 
# Boolean constraints take the form of a simple statement of the
# form [dependent_column] [operator] [expression / independent_column].
# The tool will try to guess these relationships when creating a
# spec. You can also force a column to be always smaller / larger
# than a scalar value. Note that adding a boolean constraint between
# two columns will affect the distribution of weights and also the
# target sum as these are designed to work with a single column.
#
# Custom constraints are more flexible and can target specific
# subsets of values with different actions. Currently the following
# actions are supported:
#
# - "make_null"
# - "make_not_null"
# - "make_outlier"
# - "sort_ascending"
# - "sort_descending"
# - "make_distinct"
# - "make_same"
# - "generate_as_sequence"
# - "geo_make_regions
#
# Adding or banning nulls is useful when a value in one column, 
# like Readmissions Within 28 days, necessitates a valid value in
# another, like Readmissions Within 7 days.
# 
# The format for custom constraints is as follows:
#
# demo_constraint_name:
#   filter: (los > 2)
#   partition: age, sex
#   targets:
#     taget_column: target_action
#
# If a column name has spaces, make sure to surround it with
# the tilde character ~. This rule only applies when columns are used
# in basic constraints or filters. Targets for custom constraints must 
# use column names as they are. When comparing a date column against a
# fixed date, make sure it's in an ISO format and is enclosed in
# single quotation marks like so '2018-12-01'.
# ----------------------------------------------------------
constraints:
  allow_duplicates: true
  basic_constraints:
  custom_constraints:
    multi_dose:
      partition: record_chi
      targets:
        dose_number : generate_as_sequence
    chronology:
      partition: record_chi
      targets:
        vacc_date : sort_ascending
        dose_number : sort_ascending
        sex : make_same
# ----------------------------------------------------------
# LINKED COLUMNS
# ===============
#
# There are two types of linked groups - those manually defined using
# the --linked_columns (or -lc) command line parameter and automatically
# detected groups where columns follow a hierarchical (one to many)
# relationship. User defined linked columns are always put under the
# zero indexed group.
#
# Groups of hierarchically linked columns are listed together under the 
# index starting from 1. Their details are saved in the bundled anon.db
# SQLite database. 
#
# The specification format is as follows:
# - - 1
#   - - Parent column
#   - - Child column
# - - 2
#   - - ...etc.
# It's possible to add a linked columns group manually by adding 
# a table to anon.db with the hierarchical relationships. The name
# of this table must follow the format: id_N  where id is taken 
# from the metadata section and N is the group number.
# ----------------------------------------------------------
linked_columns: []
# ----------------------------------------------------------
# DERIVED COLUMNS
# ===============
#
# You can add columns that will be calculated after the rest
# of the dataset has been generated.
#
# The calculation should be in a format that Pandas' eval()
# method can parse and understand. 
#
# For example, assuming you have Numerator column A and
# Denominator column B, you would write Rate: (A / B)
# ----------------------------------------------------------
derived_columns:
  Example column: Example calculation
